f4.cv10 <- cv.glmreg(mufun2,
family = "gaussian",
alpha = 1,  # lasso penalty
data = d)
abline(v = log(f4.cv10$lambda.optim),
col = "red",
lty = 2)
log(f4.cv10$lambda.optim)
f4.tune <- glmreg(mufun2,
family = "gaussian",
alpha = 1,
data = d,
lambda = f4.cv10$lambda.optim)
coef(f4.tune)
summary(f4.tune)
# elatic net
# nested cv is needed
a <- as.list(seq(0.1, 1.0, by = 0.1))
ncv <- sapply(a, function(a) {
print(a)
set.seed(a)
cv10 <- cv.glmreg(mufun2,
family = "gaussian",
plot.it = F,  # cv-plot
alpha = a,    # lasso penalty
data = d)
c(loglik = cv10$cv[cv10$lambda.which],
lambda = cv10$lambda.optim)
})
res <- cbind.data.frame(t(ncv), alpha = unlist(a))
res
opt <- res[which.max(res[,1]),]
opt
f5.tune <- glmreg(mufun2,
family = "gaussian",
alpha = opt$alpha,
lambda = opt$lambda,
data = d)
coef(f5.tune)
# comparing coef's
round(
cbind(ridge = coef(f3.tune),
lasso = coef(f4.tune),
elnet = coef(f5.tune),
reg2  = coef(f1)),
digits = 6
)
## Gaussian dist'n ##
ny <- 1000
nx <- 2
x <- list()
for(i in 1:nx) {
x[[i]] <- runif(ny)
}
names(x) <- sprintf("x%d", 1:nx)
x <- do.call("cbind", x)
b <- matrix(c(-nx, rep(1, nx)), ncol=1)
mu <- cbind(1,x) %*% b
sig <- 1
e <- rnorm(n=ny, mean=0, sd=sig)
y <- mu + e
hist(y)
d <- cbind.data.frame(y, x)
head(d); dim(d);
# glm
f0 <- glm(y ~ .,
family = "gaussian" (link = "identity"),
data = d)
summary(f0)
# real dataset
data(mtcars)
head(mtcars); dim(mtcars);
f1 <- glm(mpg ~ cyl + disp + hp + drat + wt + qsec,
family = "gaussian" (link = "identity"),
data = mtcars)
summary(f1)
## Gamma dist'n ##
ny <- 1000
nx <- 2
x <- list()
for(i in 1:nx) {
x[[i]] <- runif(ny)
}
names(x) <- sprintf("x%d", 1:nx)
x <- do.call("cbind", x)
x
b <- matrix(c(-nx, rep(1, nx)), ncol=1)
mu <- exp(cbind(1,x) %*% b)
th <- 2
y <- rgamma(n = ny, shape = 1/th, scale = mu*th)
hist(y)
d <- cbind.data.frame(y, x)
head(d); dim(d);
# glm
f0 <- glm(y ~ .,
family = "Gamma"(link = "log"),
data = d)
summary(f0)
summary(f0)$dispersion
# Motor Trend Car Road Tests
data(mtcars)
summary(f0)$dispersion
summary(f0)
# Motor Trend Car Road Tests
data(mtcars)
head(mtcars); dim(mtcars);
f1 <- glm(mpg ~ cyl + disp + hp + drat + wt + qsec,
family = "Gamma"(link = "log"),
data = mtcars)
summary(f1)
## Shrinkage estimation for glm ##
library(mpath)  # for ridge, lasso and elastic net
library(pscl)
data(bioChemists)
head(bioChemists); dim(bioChemists);
hist(bioChemists$art)
# glm
f0 <- glm(art ~ .,
family = "poisson",
data = bioChemists)
coef(f0)
# ridge
set.seed(1)
f1.cv10 <- cv.glmreg(art ~ .,
family = "poisson",
alpha = 0,  # ridge penalty
data = bioChemists,
lambda = seq(0.01, 1, by = 0.01))
abline(v = log(f1.cv10$lambda.optim),
col = "red",
lty = 2)
f1.cv10 <- cv.glmreg(art ~ .,
family = "poisson",
alpha = 0,  # ridge penalty
data = bioChemists,
lambda = seq(0.01, 1, by = 0.01))
abline(v = log(f1.cv10$lambda.optim),
col = "red",
lty = 2)
f1.tune <- glmreg(art ~ .,
family = "poisson",
alpha = 0,
data = bioChemists,
lambda = f1.cv10$lambda.optim)
coef(f1.tune)
# lasso
set.seed(1)
f2.cv10 <- cv.glmreg(art ~ .,
family = "poisson",
alpha = 1,  # lasso penalty
data = bioChemists)
abline(v = log(f2.cv10$lambda.optim),
col = "red",
lty = 2)
f2.tune <- glmreg(art ~ .,
family = "poisson",
alpha = 1,
data = bioChemists,
lambda = f2.cv10$lambda.optim)
coef(f2.tune)
install.packages(c("party", "rpart.plot"))
# example 1
set.seeed(1)
ny <- 100
x <- sort(rnorm(ny, 0, 2))
y <- sin(x) + rnorm(ny, 0, 0.25)
d <- cbind.data.frame(y, x)
plot(y ~ x, data = d)
library(tree)
f <- tree(y ~ x, data = d)
lines(x = x,
y = predict(f),
lty = 2, col = "red")
library(mgcv)
g <- gam(y ~ s(x), data = d)
lines(x = x,
y = predict(g),
lty = 2, col = "blue")
legend("topright",
legend = c("obs", "tree", "gam"),
inset = c(0.015, 0.015),
col = c("black", "red", "blue"),
pch = c(1,NA,NA),
lty = c(NA,2,2),
cex = 0.75)
ny <- 100
x <- sort(rnorm(ny, 0, 2))
y <- cbind(1,x) %*% c(0,1) + rnorm(ny, 0, 0.25)
d <- cbind.data.frame(y, x)
plot(y ~ x, data = d)
library(tree)
f <- tree(y ~ x, data = d)
lines(x = x,
y = predict(f),
lty = 2, col = "red")
library(mgcv)
g <- gam(y ~ s(x), data = d)
lines(x = x,
y = predict(g),
lty = 2, col = "blue")
legend("topleft",
legend = c("obs", "tree", "gam"),
inset = c(0.015, 0.015),
col = c("black", "red", "blue"),
pch = c(1,NA,NA),
lty = c(NA,2,2),
cex = 0.75)
# Regression tree
airq <- subset(airquality, !is.na(Ozone))
head(airq); str(airq);
f <- tree(Ozone ~ ., data = airq)
f
f$frame
plot(f)
# plot(f, type="uniform")
text(f, cex = 0.9)
plot(f)
text(f, cex = 0.9, all = T)
f.cv <- cv.tree(f)
plot(f.cv)
f.pr <- prune.tree(f, best = 6)
plot(f.pr)
text(f.pr, cex = 0.9)
library(rpart)
library(rpart.plot)
g <- rpart(Ozone ~ ., data = airq)
prp(g, type=4, extra = 1)
library(party)
g <- ctree(Ozone ~ ., data = airq)
plot(g)
# Classification tree
f <- tree(Species ~ ., data = iris)
f
f$frame
plot(f)
# plot(f, type="uniform")
text(f, cex = 0.9)
plot(f)
text(f, cex = 0.9, all = T)
f.cv <- cv.tree(f)
plot(f.cv)
f.pr <- prune.tree(f, best = 4)
f.pr$frame
plot(f.pr)
text(f.pr, cex = 0.9)
library(rpart)
library(rpart.plot)
g <- rpart(Species ~ ., data = iris)
prp(g, type=4, extra = 1)
library(party)
g <- ctree(Species ~ .,data = iris)
plot(g)
ny <- 100
x <- sort(rnorm(ny, 0, 2))
y <- sin(x) + rnorm(ny, 0, 0.25)
d <- cbind.data.frame(y, x)
plot(y ~ x, data = d)
install.packages("tree")
library(tree)
f <- tree(y ~ x, data = d)
lines(x = x,
y = predict(f),
lty = 2, col = "red")
install.packages("randomForest")
library(randomForest)
set.seed(1)
r <- randomForest(y ~ x, data = d)
lines(x = x,
y = predict(r),
lty = 2, col = "blue")
legend("topright",
legend = c("obs", "tree", "ensemble"),
inset = c(0.015, 0.015),
col = c("black", "red", "blue"),
pch = c(1,NA,NA),
lty = c(NA,2,2),
cex = 0.75)
# example 2
set.seed(1)
ny <- 100
x <- sort(rnorm(ny, 0, 2))
y <- cbind(1,x) %*% c(0,1) + rnorm(ny, 0, 0.25)
d <- cbind.data.frame(y, x)
plot(y ~ x, data = d)
library(tree)
f <- tree(y ~ x, data = d)
lines(x = x,
y = predict(f),
lty = 2, col = "red")
library(randomForest)
set.seed(1)
r <- randomForest(y ~ x, data = d)
lines(x = x,
y = predict(r),
lty = 2, col = "blue")
legend("topleft",
legend = c("obs", "tree", "ensemble"),
inset = c(0.015, 0.015),
col = c("black", "red", "blue"),
pch = c(1,NA,NA),
lty = c(NA,2,2),
cex = 0.75)
# example 3
library(tree)
f <- tree(Species ~ ., data = iris)
plot(f)
text(f, cex = 0.9)
for(k in 1:3) {
set.seed(k)
idx <- sample(1:nrow(iris), size = nrow(iris), replace = T)
iris.boot <- iris[idx,]
f <- tree(Species ~ ., data = iris.boot)
plot(f)
text(f, cex = 0.9)
title(paste0(k, "th boot"))
}
## loading dataset
library(MASS)  # Boston dataset
data("Boston")
dat <- Boston
str(dat)
head(dat)
## partitioning dataset
set.seed(1)
idx <- sample(x = 1:nrow(dat),
size = floor(nrow(dat) * 0.1),
replace = F)
dat_tr <- dat[-idx,]; str(dat_tr);
dat_te <- dat[+idx,]; str(dat_te);
## training
# tree
library(tree)
f.rt <- tree(medv ~ .,
data = dat_tr)
plot(f.rt)
text(f.rt, cex=0.8)
set.seed(1)
rt.tune <- cv.tree(f.rt)
plot(rt.tune)
f.rt.sub <- prune.tree(f.rt, best = 8)
plot(f.rt.sub)
text(f.rt.sub, cex = 0.9)
# bagging
library(randomForest)
set.seed(1)
f.bg <- randomForest(medv ~ .,
data = dat_tr,
mtry = 13,  # using all predictors = bagging
ntree=500,
importance =TRUE)
# random forest
set.seed(1)
rf.tune <- tuneRF(x = dat_tr[,-14],
y = dat_tr[,+14],
data = dat_tr)
rf.tune
set.seed(1)
f.rf <- randomForest(medv ~ .,
data = dat_tr,
ntree=500,
mtry = 6,  # using some predictors = r.f.
importance =TRUE)
# oob-mse
plot(f.bg,
lty=2,
main = "")
plot(f.rf, lty=2, col=2, add=T)
legend("topright",
legend = c("bagging", "random forest"),
inset = c(0.015, 0.015),
col = c("black", "red"),
lty = c(2,2),
cex = 0.75)
# varimplot
varImpPlot(f.bg)
varImpPlot(f.rf)
## prediction
pred.rt.sub <- predict(f.rt.sub, newdata = dat_te)
pred.rt <- predict(f.rt, newdata = dat_te)
pred.bg <- predict(f.bg, newdata = dat_te)
pred.rf <- predict(f.rf, newdata = dat_te)
sapply(list(rt.sub = pred.rt.sub,
rt = pred.rt,
bg = pred.bg,
rf = pred.rf),
function(pred) {
sqrt(mean((dat_te$medv - pred)^2))
})
### classification tree, bagging, random forest
## loading dataset
library(ISLR)  # Carseats dataset
data("Carseats")
dat <- Carseats
dat <- data.frame(
dat[,colnames(dat)!="Sales"],
High = ifelse(dat$Sales<=8, "No", "Yes")
)
str(dat)
head(dat)
## partitioning dataset
set.seed(1)
idx <- sample(x = 1:nrow(dat),
size = floor(nrow(dat) * 0.1),
replace = F)
dat_tr <- dat[-idx,]; str(dat_tr);
dat_te <- dat[+idx,]; str(dat_te);
## training
# tree
library(tree)
f.rt <- tree(High ~ .,
data = dat_tr)
plot(f.rt)
## training
# tree
library(tree)
f.rt <- tree(High ~ .,
data = dat_tr)
plot(f.rt)
text(f.rt, cex=0.8)
set.seed(4)
rt.tune <- cv.tree(f.rt)
plot(rt.tune)
f.rt.sub <- prune.tree(f.rt, best = 10)
plot(f.rt.sub)
text(f.rt.sub, cex = 0.9)
# bagging
library(randomForest)
set.seed(1)
f.bg <- randomForest(High ~ .,
data = dat_tr,
mtry = 10,  # using all predictors = bagging
ntree=500,
importance =TRUE)
# random forest
set.seed(1)
rf.tune <- tuneRF(x = dat_tr[,-11],
y = dat_tr[,+11],
data = dat_tr)
rf.tune
set.seed(1)
f.rf <- randomForest(High ~ .,
data = dat_tr,
ntree=500,
importance =TRUE)
# oob-err
plot(f.bg$err.rate[,1],
type = "l",
lty=2,
main = "",
xlab = "trees",
ylab = "Error")
## training
# tree
library(tree)
f.rt <- tree(High ~ .,
data = dat_tr)
plot(f.rt)
text(f.rt, cex=0.8)
set.seed(4)
rt.tune <- cv.tree(f.rt)
plot(rt.tune)
f.rt.sub <- prune.tree(f.rt, best = 10)
plot(f.rt.sub)
text(f.rt.sub, cex = 0.9)
# bagging
library(randomForest)
set.seed(1)
f.bg <- randomForest(High ~ .,
data = dat_tr,
mtry = 10,  # using all predictors = bagging
ntree=500,
importance =TRUE)
# random forest
set.seed(1)
rf.tune <- tuneRF(x = dat_tr[,-11],
y = dat_tr[,+11],
data = dat_tr)
rf.tune
set.seed(1)
f.rf <- randomForest(High ~ .,
data = dat_tr,
ntree=500,
importance =TRUE)
# oob-err
plot(f.bg$err.rate[,1],
type = "l",
lty=2,
main = "",
xlab = "trees",
ylab = "Error")
points(f.rf$err.rate[,1], type = "l", lty=2, col=2)
legend("topright",
legend = c("bagging", "random forest"),
inset = c(0.015, 0.015),
col = c("black", "red"),
lty = c(2,2),
cex = 0.75)
# varimplot
varImpPlot(f.bg)
varImpPlot(f.rf)
## prediction
pred.rt.sub <- predict(f.rt.sub, newdata = dat_te, type = "class")
pred.rt <- predict(f.rt, newdata = dat_te, type = "class")
pred.bg <- predict(f.bg, newdata = dat_te, type = "response")
pred.rf <- predict(f.rf, newdata = dat_te, type = "response")
lapply(list(rt.sub = pred.rt.sub,
rt = pred.rt,
bg = pred.bg,
rf = pred.rf),
function(pred) {
table(dat_te$High, pred)
})
